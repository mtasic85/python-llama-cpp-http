[tool.poetry]
name = "llama_cpp_http"
version = "0.1.1"
description = "Python llama.cpp HTTP Server and LangChain LLM Client"
authors = ["Marko Tasic <mtasic85@gmail.com>"]
license = "MIT"
readme = "README.md"
packages = [{include = "llama_cpp_http"}]
homepage = "https://github.com/mtasic85/python-llama-cpp-http"
repository = "https://github.com/mtasic85/python-llama-cpp-http"

[tool.poetry.extras]

[tool.poetry.dependencies]
python = "^3.10"
aiohttp = {extras = ["speedups"], version = "^3.8.5"}
async-timeout = "^4.0.3"
websockets = "^11.0.3"
requests = "^2.31.0"
langchain = "^0.0.268"
pony = "^0.7.16"
gunicorn = "^21.2.0"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
